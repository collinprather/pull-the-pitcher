{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "\n",
    "> Command-line script, which utilizes the `data_processing` module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti \n",
    "from pull_the_pitcher.data import processing\n",
    "from pull_the_pitcher.data.acquisition import query_db\n",
    "from pull_the_pitcher.data.processing import last\n",
    "from fastscript import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "embedding_cols = [\"game_pk\", \"game_type\", \"pitcher\", \"pitcher_team_year\"]\n",
    "feature_cols = [\"post_bat_score\", \"score_diff\", \"end_inning\", \"inning\", \"postouts\", \"cum_sb_ratio\",\n",
    "                \"times_thru_order\", \"post_total_runners\", \"tying_run_on\", \"pitch_total\", \"post_opposite_hand\",\n",
    "                \"walk\", 'walk_cumsum', 'strikeout_cumsum', 'home_run_cumsum', 'bases_cumsum']\n",
    "cols = embedding_cols + feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# adding targets to each\n",
    "def add_targets(starts: List):\n",
    "    \"\"\"adding target as last col to each start\"\"\"\n",
    "    for i, start in enumerate(starts):\n",
    "        y = np.zeros((start.shape[0], 1))\n",
    "        y[-1, 0] = 1\n",
    "        starts[i] = np.concatenate([start, y], axis=1)\n",
    "    return starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def stack_into_df(starts: List):\n",
    "    # concatenating into big dfs\n",
    "    df = pd.DataFrame(np.concatenate(starts, axis=0), columns=cols+[\"pulled\"])\n",
    "\n",
    "    # correcting data types\n",
    "    for col in feature_cols + [\"pulled\"]:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def scale(train: pd.DataFrame, val: pd.DataFrame):\n",
    "    # scaling data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train[feature_cols])\n",
    "    train[feature_cols] = scaler.transform(train[feature_cols])\n",
    "    val[feature_cols] = scaler.transform(val[feature_cols])\n",
    "    return train, val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def encode_col(train, valid, col=\"pitcher_id\"):\n",
    "    \n",
    "    # encoding movies and user ids with continous ids\n",
    "    train_ids = np.sort(np.unique(train[col].values))\n",
    "\n",
    "    # number of unique ids\n",
    "    num_users = len(train_ids)\n",
    "    print(f\"There are {num_users} unique {col}'s in this dataset\")\n",
    "\n",
    "    # making changes in df\n",
    "    id2idx = {o:i for i,o in enumerate(train_ids)}\n",
    "    train[col] = train[col].apply(lambda x: id2idx[x])\n",
    "    valid[col] = valid[col].apply(lambda x: id2idx.get(x, -1)) # -1 for users not in training\n",
    "    \n",
    "    # getting rid of users not in training set\n",
    "    valid = valid[valid[col] >= 0].copy()\n",
    "    return train, valid, id2idx\n",
    "\n",
    "\n",
    "def encode_embedding_cols(train, val, cols=[\"game_pk\", \"game_type\", \"pitcher\", \"pitcher_team_year\"]):\n",
    "    # adding a row of zeros that act as \"null\" or \"unknown\"\n",
    "    # embeddings for the zero-padded rows\n",
    "    zero_row = pd.DataFrame(np.zeros((1, train.shape[1])), columns=train.columns)\n",
    "    train = pd.concat([zero_row, train], axis=0)\n",
    "    val = pd.concat([zero_row, val], axis=0)\n",
    "\n",
    "    # changing dtypes in order to encode for embeddings\n",
    "    for cat in [\"game_type\", \"pitcher_team_year\"]:\n",
    "        train[cat] = train[cat].astype(str)\n",
    "        val[cat] = val[cat].astype(str)\n",
    "        \n",
    "    mappers = dict()\n",
    "    # not embedding game_pk, just switching to int for easier type casting\n",
    "    for col in cols:\n",
    "        train, val, mapper = encode_col(train, val, col=col)\n",
    "        mappers[col] = mapper\n",
    "    \n",
    "    return train, val, mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "at_bat_aggs = {\n",
    "    \"balls\": \"max\",\n",
    "    \"strikes\": \"max\",\n",
    "    \"pitch_number\": \"max\",\n",
    "    \"post_bat_score\": last,\n",
    "    \"post_fld_score\": last,\n",
    "    \"events\": \"max\",\n",
    "    \"postouts\": last,\n",
    "    \"post_on_1b\": last,\n",
    "    \"post_on_2b\": last,\n",
    "    \"post_on_3b\": last,\n",
    "    \"game_type\": last,\n",
    "    \"home_team\": last,\n",
    "    \"away_team\": last,\n",
    "    \"inning\": last,\n",
    "    \"inning_topbot\": last,\n",
    "    \"post_opposite_hand\": last,\n",
    "    \"game_year\": last,\n",
    "    \"pitcher_team\": last\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def add_val_flags(ds_flags: List[int]):\n",
    "    \"\"\"\n",
    "    Adds some 1's to the list of dataset flags to move\n",
    "    a random 15% of the training set into the validation\n",
    "    set\n",
    "    \"\"\"\n",
    "    ds_flags = np.array(ds_flags)\n",
    "    total_train = (ds_flags == 0).sum()\n",
    "    val_flags = bernoulli(p=0.15).rvs(size=total_train, random_state=742)\n",
    "    val_indx = 0\n",
    "    for i, flag in enumerate(ds_flags):\n",
    "        if flag == 0:\n",
    "            ds_flags[i] = val_flags[val_indx]\n",
    "            val_indx += 1\n",
    "    return ds_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def prep_data_for_modeling(\n",
    "    db_path: Param(\n",
    "        help=\"Path to db with statcast data\", type=str\n",
    "    ) = \"./data/raw/statcast_pitches.db\",\n",
    "    years: Param(help=\"Year of statcast data to process\", type=str, nargs=\"+\") = [\n",
    "        \"2019\"\n",
    "    ],\n",
    "    verbose: Param(\n",
    "        help=\"Whether to print out updates on processing\", type=bool_arg\n",
    "    ) = True,\n",
    "    train_test_split_by: Param(\n",
    "        help=\"How to split into train/test sets. One of {'start', 'year'}.\", type=str\n",
    "    ) = \"start\",\n",
    "    test_size: Param(help=\"Percent of data to allocate to test set\", type=float) = 0.25,\n",
    "    output_path: Param(\n",
    "        help=\"Path to save processed csv files\", type=str\n",
    "    ) = \"./data/processed/\",\n",
    "):\n",
    "    # getting all dfs from all years into a single df\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        df_year = query_db(db_path, year, verbose=verbose)\n",
    "        dfs.append(df_year)\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "\n",
    "    # identifying eligible game-pitcher-year combos\n",
    "    games_pitchers_years = processing.get_games_pitchers_years(df, verbose)\n",
    "\n",
    "    # deciding which outings to allocate to train or test set\n",
    "    # 2 is test, 1 is val, 0 is train\n",
    "    if train_test_split_by == \"start\":\n",
    "        # pre-determining which starts will go into train/test sets\n",
    "        ds_flags = bernoulli(p=test_size).rvs(\n",
    "            len(games_pitchers_years), random_state=742\n",
    "        ) * 2\n",
    "        train_year = test_year = years\n",
    "    elif train_test_split_by == \"year\":\n",
    "        # identifying year of test starts\n",
    "        test_year = [np.sort(df[\"game_date\"].str[:4].unique())[-1]]\n",
    "        train_year = list(set(years).difference(set(test_year)))\n",
    "        ds_flags = [\n",
    "            2 if str(y) == test_year[0] else 0 for (g, p, y) in games_pitchers_years\n",
    "        ]\n",
    "    else:\n",
    "        # no starts go to test set\n",
    "        test_flags = np.zeros(len(games_pitchers))\n",
    "    \n",
    "    # making %15 of training set be val set\n",
    "    ds_flags = add_val_flags(ds_flags)\n",
    "\n",
    "    # processing dfs of data from eligible pitchers\n",
    "    train_starts = []\n",
    "    val_starts = []\n",
    "    test_starts = []\n",
    "    for i, (ds_flag, (g, p, y)) in enumerate(zip(ds_flags, games_pitchers_years)):\n",
    "        if verbose:\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Just processed {i}th start.\")\n",
    "\n",
    "        cleaned_df = processing.preliminary_clean(df, g, p)\n",
    "        agged_df = processing.aggregate_at_bats(cleaned_df, at_bat_aggs)\n",
    "        feature_engineered_df = processing.feature_engineering(agged_df)\n",
    "\n",
    "#         # making sure starting pitcher is in AL -> this _should_ no longer be necessary\n",
    "#         if feature_engineered_df.shape[0] > 0:\n",
    "        if ds_flag == 2:\n",
    "            test_starts.append(feature_engineered_df[cols])\n",
    "        elif ds_flag == 1:\n",
    "            val_starts.append(feature_engineered_df[cols])\n",
    "        else:\n",
    "            train_starts.append(feature_engineered_df[cols])\n",
    "#         else:\n",
    "#             print(\"empty df\")\n",
    "\n",
    "    # adding binary targets (pitcher always removed in last at-bat)\n",
    "    train_starts = add_targets(train_starts)\n",
    "    val_starts = add_targets(val_starts)\n",
    "    test_starts = add_targets(test_starts)\n",
    "\n",
    "    # stacking starts into dfs for scaling and categorical encoding\n",
    "    train = stack_into_df(train_starts)\n",
    "    val = stack_into_df(val_starts)\n",
    "    test = stack_into_df(test_starts)\n",
    "\n",
    "    # standard scaling (mean of 0, sd of 1)\n",
    "    train, val, scaler = scale(train, val)\n",
    "\n",
    "    # encoding categoricals for embeddings later\n",
    "    train, val, mappers = encode_embedding_cols(train, val, cols=[\"pitcher\"])\n",
    "\n",
    "    # saving train, val, test sets, along with categorical mapper and scaler to output path\n",
    "    train.to_csv(\n",
    "        f\"{output_path}/train_{'_'.join(train_year)}.csv\", index=False)\n",
    "    val.to_csv(f\"{output_path}/val_{'_'.join(train_year)}.csv\", index=False)\n",
    "    test.to_csv(f\"{output_path}/test_{'_'.join(test_year)}.csv\", index=False)\n",
    "\n",
    "    with open(\n",
    "        f\"{output_path}/mappers_{'_'.join(train_year + test_year)}.pkl\", \"wb\"\n",
    "    ) as f:\n",
    "        pickle.dump(mappers, f)\n",
    "\n",
    "    with open(\n",
    "        f\"{output_path}/scaler_{'_'.join(train_year + test_year)}.pkl\", \"wb\"\n",
    "    ) as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{years} data ready for modeling and saved at {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /tmp/*.db /tmp/*.pkl /tmp/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a large query, it may take a moment to complete\n",
      "Completed sub-query from 2018-07-07 to 2018-07-12\n",
      "Completed sub-query from 2018-07-13 to 2018-07-18\n",
      "Completed sub-query from 2018-07-19 to 2018-07-20\n",
      "Table named 'statcast_2019' already exists in db saved at `/tmp/statcast_2019`.\n"
     ]
    }
   ],
   "source": [
    "! query_statcast --start_dt 2018-07-07 --end_dt 2018-07-20 --output_type db --output_path /tmp\n",
    "! query_statcast --start_dt 2019-07-07 --end_dt 2019-07-20 --output_type db --output_path /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adobesmuoutpJskbsk           \u001b[34mcom.apple.launchd.G3SsF3Afuq\u001b[m\u001b[m\n",
      "adobesmuoutpV1Fvr8           \u001b[34mcom.google.Keystone\u001b[m\u001b[m\n",
      "adobesmuoutpXErfzF           \u001b[34mpowerlog\u001b[m\u001b[m\n",
      "adobesmuoutprupIXQ           statcast_pitches.db\n"
     ]
    }
   ],
   "source": [
    "! ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying db at /tmp/statcast_pitches.db now.\n",
      "querying db at /tmp/statcast_pitches.db now.\n",
      "In this dataset, there are 290 total games.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinprather/anaconda3/envs/orioles/lib/python3.7/site-packages/pandas/core/indexing.py:845: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/collinprather/anaconda3/envs/orioles/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 339 ineligible starts in the dataset (either 'openers' or an NL team).\n",
      "There are 241 total eligible game-pitcher combinations in this dataset.\n",
      "Just processed 0th start.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinprather/Documents/Baltimore-Orioles/pull-the-pitcher/notebooks/pull_the_pitcher/data/processing.py:193: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  game_df[\"pitcher_team\"] = game_df.apply(lambda row: add_pitcher_team(row), axis=1)\n",
      "/Users/collinprather/Documents/Baltimore-Orioles/pull-the-pitcher/notebooks/pull_the_pitcher/data/processing.py:207: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  game_team_pitcher_df[\"events\"] = game_team_pitcher_df[\"events\"].fillna(\"\")\n",
      "/Users/collinprather/Documents/Baltimore-Orioles/pull-the-pitcher/notebooks/pull_the_pitcher/data/processing.py:210: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  game_team_pitcher_df[\"post_bat_score\"] = game_team_pitcher_df[\"post_bat_score\"].shift(-1).fillna(method=\"ffill\")\n",
      "/Users/collinprather/Documents/Baltimore-Orioles/pull-the-pitcher/notebooks/pull_the_pitcher/data/processing.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  game_team_pitcher_df[f\"post_on_{base}b\"] = game_team_pitcher_df[f\"on_{base}b\"].fillna(0).apply(lambda x: 1 if x>0 else 0).shift(-1).fillna(method=\"ffill\")\n",
      "/Users/collinprather/Documents/Baltimore-Orioles/pull-the-pitcher/notebooks/pull_the_pitcher/data/processing.py:217: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  game_team_pitcher_df[\"post_opposite_hand\"] = (game_team_pitcher_df[\"stand\"]!=game_team_pitcher_df[\"p_throws\"]).astype(int).shift(-1).fillna(method=\"ffill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just processed 100th start.\n",
      "Just processed 200th start.\n",
      "There are 72 unique pitcher's in this dataset\n",
      "['2018', '2019'] data ready for modeling and saved at /tmp.\n"
     ]
    }
   ],
   "source": [
    "prep_data_for_modeling(db_path=\"/tmp/statcast_pitches.db\",\n",
    "                       years=[\"2018\", \"2019\"],\n",
    "                       train_test_split_by=\"year\",\n",
    "                       output_path=\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adobesmuoutpJskbsk           \u001b[34mpowerlog\u001b[m\u001b[m\n",
      "adobesmuoutpV1Fvr8           scaler_2018_2019.pkl\n",
      "adobesmuoutpXErfzF           statcast_pitches.db\n",
      "adobesmuoutprupIXQ           test_2019.csv\n",
      "\u001b[34mcom.apple.launchd.G3SsF3Afuq\u001b[m\u001b[m train_2018.csv\n",
      "\u001b[34mcom.google.Keystone\u001b[m\u001b[m          val_2018.csv\n",
      "mappers_2018_2019.pkl\n"
     ]
    }
   ],
   "source": [
    "! ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /tmp/*.db /tmp/*.pkl /tmp/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sanity Check\n",
    "\n",
    "To make sure there is no crazy data leakage, I make sure that a baseline logistic regression classifier gets a recall score of < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/tmp/train_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[feature_cols]\n",
    "y = train[\"pulled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98      2358\n",
      "         1.0       0.84      0.25      0.39       104\n",
      "\n",
      "    accuracy                           0.97      2462\n",
      "   macro avg       0.90      0.62      0.68      2462\n",
      "weighted avg       0.96      0.97      0.96      2462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, log_reg.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert recall_score(y, log_reg.predict(X)) < 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
