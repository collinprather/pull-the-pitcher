{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "\n",
    "> Command-line script, which utilizes the `data_processing` module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti \n",
    "from pull_the_pitcher.data import processing\n",
    "from fastscript import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "embedding_cols = [\"game_pk\", \"game_type\", \"pitcher\", \"pitcher_team_year\"]\n",
    "feature_cols = [\"post_bat_score\", \"score_diff\", \"end_inning\", \"inning\", \"postouts\", \"cum_sb_ratio\",\n",
    "                \"times_thru_order\", \"post_total_runners\", \"tying_run_on\", \"pitch_total\", \"post_opposite_hand\",\n",
    "                \"walk\", 'walk_cumsum', 'strikeout_cumsum', 'home_run_cumsum', 'bases_cumsum']\n",
    "cols = embedding_cols + feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# adding targets to each\n",
    "def add_targets(starts: List):\n",
    "    \"\"\"adding target as last col to each start\"\"\"\n",
    "    for i, start in enumerate(starts):\n",
    "        y = np.zeros((start.shape[0], 1))\n",
    "        y[-1, 0] = 1\n",
    "        starts[i] = np.concatenate([start, y], axis=1)\n",
    "    return starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def stack_into_df(starts: List):\n",
    "    # concatenating into big dfs\n",
    "    df = pd.DataFrame(np.concatenate(starts, axis=0), columns=cols+[\"pulled\"])\n",
    "\n",
    "    # correcting data types\n",
    "    for col in feature_cols + [\"pulled\"]:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def scale(train: pd.DataFrame, test: pd.DataFrame):\n",
    "    # scaling data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train[feature_cols])\n",
    "    train[feature_cols] = scaler.transform(train[feature_cols])\n",
    "    test[feature_cols] = scaler.transform(test[feature_cols])\n",
    "    return train, test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def encode_col(train, valid, col=\"pitcher_id\"):\n",
    "    \n",
    "    # encoding movies and user ids with continous ids\n",
    "    train_ids = np.sort(np.unique(train[col].values))\n",
    "\n",
    "    # number of unique ids\n",
    "    num_users = len(train_ids)\n",
    "    print(f\"There are {num_users} unique {col}'s in this dataset\")\n",
    "\n",
    "    # making changes in df\n",
    "    id2idx = {o:i for i,o in enumerate(train_ids)}\n",
    "    train[col] = train[col].apply(lambda x: id2idx[x])\n",
    "    valid[col] = valid[col].apply(lambda x: id2idx.get(x, -1)) # -1 for users not in training\n",
    "    \n",
    "    # getting rid of users not in training set\n",
    "    valid = valid[valid[col] >= 0].copy()\n",
    "    return train, valid, id2idx\n",
    "\n",
    "\n",
    "def encode_embedding_cols(train, test, cols=[\"game_pk\", \"game_type\", \"pitcher\", \"pitcher_team_year\"]):\n",
    "    # adding a row of zeros that act as \"null\" or \"unknown\"\n",
    "    # embeddings for the zero-padded rows\n",
    "    zero_row = pd.DataFrame(np.zeros((1, train.shape[1])), columns=train.columns)\n",
    "    train = pd.concat([zero_row, train], axis=0)\n",
    "    test = pd.concat([zero_row, test], axis=0)\n",
    "\n",
    "    # changing dtypes in order to encode for embeddings\n",
    "    for cat in [\"game_type\", \"pitcher_team_year\"]:\n",
    "        train[cat] = train[cat].astype(str)\n",
    "        test[cat] = test[cat].astype(str)\n",
    "        \n",
    "    mappers = dict()\n",
    "    # not embedding game_pk, just switching to int for easier type casting\n",
    "    for col in cols:\n",
    "        train, test, mapper = encode_col(train, test, col=col)\n",
    "        mappers[col] = mapper\n",
    "    \n",
    "    return train, test, mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "@call_parse\n",
    "def prep_data_for_modeling(db_path: Param(help=\"Path to db with statcast data\", \n",
    "                                          type=str)=\"./data/raw/statcast_pitches.db\",\n",
    "                          years: Param(help=\"Year of statcast data to process\", \n",
    "                                      type=str,\n",
    "                                      nargs=\"+\")=[\"2019\"],\n",
    "                          verbose: Param(help=\"Whether to print out updates on processing\",\n",
    "                                        type=bool_arg)=True,\n",
    "                          train_test_split_by: Param(help=\"How to split into train/test sets. One of {'start', 'year'}.\",\n",
    "                                                  type=str)=\"start\",\n",
    "                          test_size: Param(help=\"Percent of data to allocate to test set\",\n",
    "                                          type=float)=0.25,\n",
    "                          output_path: Param(help=\"Path to save processed csv files\",\n",
    "                                             type=str)=\"./data/processed/\"):\n",
    "    # getting all dfs from all years into a single df\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        df_year = processing.query_db(db_path, year, verbose=verbose)\n",
    "        dfs.append(df_year)\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    \n",
    "    # identifying eligible game-pitcher-year combos\n",
    "    games_pitchers_years = processing.get_games_pitchers_years(df, verbose)\n",
    "    \n",
    "    # deciding which outings to allocate to train or test set\n",
    "    if train_test_split_by == \"start\":\n",
    "        # pre-determining which starts will go into train/test sets\n",
    "        test_flags = bernoulli(p=test_size).rvs(len(games_pitchers_years), random_state=742)\n",
    "        train_year = test_year = years\n",
    "    elif train_test_split_by == \"year\":\n",
    "        # identifying year of test starts\n",
    "        test_year = list(np.sort(df[\"game_date\"].str[:4].unique())[-1])\n",
    "        train_year = list(set(years).difference(set([test_year])))\n",
    "        test_flags = [1 if str(y)==test_year[0] else 0 for (g, p, y) in games_pitchers_years]\n",
    "    else:\n",
    "        # no starts go to test set\n",
    "        test_flags = np.zeros(len(games_pitchers))\n",
    "    \n",
    "    # processing dfs of data from eligible pitchers\n",
    "    train_starts = []\n",
    "    test_starts = []\n",
    "    for i, (test_flag, (g, p, y)) in enumerate(zip(test_flags, games_pitchers_years)):\n",
    "        if verbose:\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Just processed {i}th start.\")\n",
    "            \n",
    "        cleaned_df = processing.preliminary_clean(df, g, p)\n",
    "        agged_df = processing.aggregate_at_bats(cleaned_df)\n",
    "        feature_engineered_df = processing.feature_engineering(agged_df)\n",
    "        \n",
    "        # making sure starting pitcher is in AL\n",
    "        if feature_engineered_df.shape[0] > 0:\n",
    "            if test_flag:\n",
    "                test_starts.append(feature_engineered_df[cols])\n",
    "            else:\n",
    "                train_starts.append(feature_engineered_df[cols])\n",
    "    \n",
    "    # adding binary targets (pitcher always removed in last at-bat)\n",
    "    train_starts = add_targets(train_starts)\n",
    "    test_starts = add_targets(test_starts)\n",
    "    \n",
    "    # stacking starts into dfs for scaling and categorical encoding\n",
    "    train = stack_into_df(train_starts)\n",
    "    test = stack_into_df(test_starts)\n",
    "    \n",
    "    # standard scaling (mean of 0, sd of 1)\n",
    "    train, test, scaler = scale(train, test)\n",
    "    \n",
    "    # encoding categoricals for embeddings later\n",
    "    train, test, mappers = encode_embedding_cols(train, test, cols=[\"pitcher\"])\n",
    "    \n",
    "    # saving train, test sets, along with categorical mapper to output path\n",
    "    train.to_csv(f\"{output_path}/train_{'_'.join(train_year)}.csv\", index=False)\n",
    "    test.to_csv(f\"{output_path}/test_{'_'.join(test_year)}.csv\", index=False)\n",
    "    with open(f\"{output_path}/mappers_{'_'.join(train_year + test_year)}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(mappers, f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{years} data ready for modeling and saved at {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_data_for_modeling(db_path=\"../data/raw/statcast_pitches.db\",\n",
    "#                        years=[\"2018\", \"2019\"],\n",
    "#                        train_test_split_by=\"year\",\n",
    "#                        output_path=\"/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls /tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
